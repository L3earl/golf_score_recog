## 테스트 남은 이슈
- 현황: '이글' 표시와 같은 얼룩이 있는 숫자를 인식시키기 위해, 검은색 외의 모든 색상을 제거하는 전처리 로직이 적용되어 있습니다.
 
필요 작업: 현재 -1 케이스만 테스트된 상태입니다. 버디(새), 더블보기(네모) 등 다른 종류의 아이콘이나 얼룩이 있는 경우에도 이 로직이 유효한지 다양한 케이스에 대한 테스트가 필요합니다.

필요 작업: -1 외에 -2, -3, -4 등 다른 음수 값도 정확히 인식하는지 테스트가 필요합니다.

필요 작업: Claude API의 CSV 형식 응답이 다양한 스코어카드 레이아웃에서 일관되게 작동하는지 검증이 필요합니다.
 

## 주요 이슈
- GPU가 필요합니다. CPU로는 속도가 나지 않음

현재 매 처리마다 모델을 GPU 메모리에 로드하고 있습니다. 이 시간이 전체 시간의 50% 정도를 차지합니다. 실제 서비스에서는 모델을 한 번만 GPU 메모리에 올린 후, 여러 요청을 메모리 상에서 계속 처리하도록 아키텍처를 변경해야 합니다.

- TrOCR 모델 한계점 : microsoft/trocr-large-printed 모델은 숫자 인식률이 높지만, 세 자릿수 숫자('100' 이상)를 인식하지 못하는 문제가 있습니다.

임시 해결책: 세 자릿수는 주로 'SUM' 열에만 등장하므로, 현재는 SUM 열의 데이터를 사용하지 않는 방식으로 우회하고 있습니다.

- LLM 프롬프트 최적화

필요 작업: 스코어카드의 양식(앱 종류, 레이아웃 등)이 다양하므로, 현재 프롬프트가 특정 케이스에서는 최적이 아닐 수 있습니다. 여러 종류의 이미지에 일관적으로 높은 성능을 내는 범용 프롬프트를 찾는 연구 및 테스트가 필요합니다.

- 모든 프로세스를 통과하지 못 한 경우, 어떻게 할지 얘기해봐야 함

- debug위한 코드들 지워야 함




# 251014
지금 코드는 GPU한개의 처리 능력을 다 쓰는거 같은데? 여러개 동시에 들어오면 훨씬 느려짐. 실서비스를 위해 속도 늘리려면 어떻게 하나
1. 좋은 GPU를 여러장 사서 분산처리 한다
2. 실시간으로 서비스를 제공하지 않는다
3. 클로드 API 2번째 버전 (가격 개당 15원 나오는 거)를 프롬프트 튜닝 잘 해서 쓴다
4. 딥러닝 모델이 아닌 아주 옛날 CV 방식을 사용한다 (아마 빠를 듯? )- 다만 2자리수, 음수는 인식 못하는 문제 풀어야 함
4-1. 좌표에 맞춰서 2자리수, 음수를 분리하여 1자리수 양수로 만든 후에 적용 (초 하드코딩)
4-2. 2자리수는 포기하고, 음수만 학습시킨 버전을 다시 만듦 (2자리수는 중요도가 높진 않은데다가, 학습을 위해서는 10~99?까지 가능한 수의 디자인이 다 있어야 함)
5. 지금 모델보다 빠르고, 정확도는 100% 나오는 모델을 노가다로 찾는다 (결과가 나올 수 있을지 없을지 명확하지 않음)


# 이슈
- 속도를 조금이라도 증가시키려면 중요도가 낮은 total, sum 빼버리기

할일
케이스3 box 좌표 알아내서 crop 하기 -> 좌표 따기 -> crop & clean 잘 되나 확인
케이스2번은 crop &clean 잘되나 확인
최적화는 일단 나중에 생각하고, main.py돌리면 3개 모델 다 돌아가게 해서 아래 구글sheet 결과지에 정보 올리기,CSV로 만들어서 주기
구글 sheet 에 각 case별로 정답지 만들기
구글 sheet 연결해서, main.py 돌릴때마다 결과지에 정보 올리게 하고, 정답지랑 비교해서 자동으로 정확도 나오게 하기 (매번 눈으로 보기 빡세니까)
3개 케이스에서 postprocess돌려서 하나라도 통과하면 cluade를 호출할 필요 없음


config에 case3이 추가되야 작업한다고? 이건 수정 필요



할일1017
클로드 프롬프트를 조절해서 원하는 데이터만 추출하는게 어려움 (프롬프트를 아무리 변경해도 결과물이 잘 변하지 않고 있음)
스코어 카드는 너무 여러 형식임

방법1
이미지+프롬프트로 고급 모델을 써서 데이터를 추출
위 결과물 + 프롬프트로, 작은 모델을 써서 데이터 필터링과 형태를 맞춤


방법2
초 하드코인
구글에 스코어 카드라고 쳐서 나오는 모든 경우의 수에 맞춰 결과물을 하나씩정제하도록 함 (찝찝한 방법임)

 _parse_csv_to_dataframe에서 CSV 문자열을 pandas DataFrame으로 파싱 한 이후에 해당 데이터프레임을 아래와 같이 조작할꺼야
- 1열 데이터가 모두 문자거나 비어있다면, 1열 삭제
- 1열이 숫자이면서, 10열의 데이터가 2자리수 숫자라면 -> 해당 열은 뒤에서 3번쨰로 옮김
- 1번 데이터가 1이고, 숫자가 1씩 증가하는게 5번 이상 반복되는 행은 삭제
- 

행보다 열이 길면 t
100이상의 숫자들이 대다수면 행 삭제
1열 인덱스를 기준으로 특정 단어들은 따로 뺴둠 (par -> 파 정보, Hole -> 지울 것, 등등)


1018 인사이트
케이스3이 4개로 분화됨에 따라 전체 케이스가 7개가 되었음 (case1,2,3_1~3_4,예외)
좌표를 6개를 따야함
하드코딩과 별의별 설정값들이 난무하여, 유지보수할때 왜 이렇게 했지? 하고 이해 못하는 케이스들이 늘어남 -> 병신 코드가 됨 -> 케이스가 늘어날때마다 하드코딩이 늘어남
접근 방식이 잘 못 되었음을 느낌
테이블을 자름 -> 테이블 내 숫자를 한번에 인식 
이렇게 2가지로 task을 나누면 좋았을 것 같다, 이렇게하면 정말 예외적인 케이스 외에는 하드코딩할 필요 없음. 나중에 개선하게 되면 고려해 볼 것
물론 현재 시점에 위 2개 task는 (왜인지는 완벽히 이해하지는 못했지만) 최신 모델로도 잘 안되고는 있다. 연구가 필요함
코딩시작하기 전 조사&분석&기획에 좀 더 시간을 썼어야 했다고 봄 (어디가 어려운가? 이렇게 진행하면 어떤 결과가 나올지? 고민을 미리했으면 좋알승ㄹ 듯 근데 뭐 지금은 경험 자체가 부족하니 어떨 수 없었을지도)

-> 아이디어 (case3에 대해, 일단 table인식은 했음. 숫자를 통으로 가져오는게 안되는건 문자(특히 한글)와 얼룩의 문제인거 같으니, 문자쪽 제외하고 crop, clean 후에 데이터 뽑아보자)
이상한건 HOLE정보는 무시되네? 모델 특성일지도..

방향성에 대한 고민
1. 최신 범용 모델을 먼저 테스트한다 (이건 잘 했음)
위에서 잘 못하면 최신 모델로도 어려운 task 라는 뜻
2. 왜 최신 범용 모델들이 잘 못하는가? (를 먼저 공부해야 했음)
이 task를 모델이 못하는 이유는 무엇인가? 
이 task를 쪼개서 특화된 모델들을 써서, 결과를 내고, 그결과를 합친다고 할때, 어느 정도까지는 모델들이 할 수 있나? (=그 task를 하는 모델이 존재하는가)
즉, OCR을 잘 하려면 어떻게 해야 하지? OCR모델들의 동작방식 (레이아웃)과 제약점 같은건 뭔지 알았어야 함
3. 위에서 최대한 적게 쪼갠 task를 하는 특화 모델로 테스트해봐야 함
그리고 안되면 이유 찾고, 더 쪼개고 했어야 함
하드코딩 (설정값 포함)이 가장 적은 방식을 생각해서 했어야 함

+ 여러 케이스를 미리 상상하거나 수집했어야 함


case3의 OCR을 이용한 1차 table crop이 잘 되면, 나중에는 case1,2 (고정된 이미지)도 좌표 방식 없애고 이 방식으로 통일해도 될 듯 

단일 라인 최적..같은 거 레이아웃이 문제라는 것 등 모델의특장점을 쓰기 전에 알았으면 좋았을듯


모든 로직을 이해하고 있지 않음 -> 다양한 케이스 테스트가 필요 

남은 할일
- reame와 할일/인사이트로 분화 - 이미지 퀄리티에 따라 인식이 안 될 수 있다는 점
- 다 완료하고 scoring까지 재 테스트
- 마지막에 리펙토링
- 99 예외처리, 여러 케이스 모아서 테스트하기
- readme에 과정 정리, 파일 설명


- case1,2는 고정된 이미지니까, 크기나 비율로 필터링 할 수 있을 듯 함. (어짜피 크기/비율 달라지면 고정 좌표가 달라져서 처리 못함)
로깅 인사이트랑 할일 분리, 배운점, 에이아이쓰는법 룰 업데이트 할 섯

작동방식을 알고, 내 예상과 빗나간 부분과의 차이를 비교했어야 했다! (-1과 4의 차이)


리펙토링 룰도 만들어야겠다
- 디버깅 해야 하는 부분에 로깅시스템 적용 (만약 print가 있다면 변경)
- 구글 스타일의 닥스트링 달기 (해당 파일, 함수, 객체가 만들어진 의도도 달기)
- 설정 및 민감 정보 분리 (환경설정, config나 .env로 옮길 수 있는 하드코딩 한 것들 옮기기)
- 불필요한 코드 삭제 (Clean Code, 지금 안 쓰는 코드, 주석처리된 코드)
- 중복 코드 제거 (DRY 원칙)
- 함수 및 클래스 분리 (SRP 원칙)

중요한 것은 
- 기능(동작)을 절대 변경하지 않는다 (The Golden Rule)
리펙토링 전과 후의 실행 결과가 100% 동일해야 함
- KISS (Keep It Simple, Stupid) 단순하게 
- YAGNI (You Ain't Gonna Need It): '나중에 필요할 것 같아서' 미리 기능을 만들지 않음
- 한번에 다 하려 하지 말고 작은 단위로 나누어서 할 것 